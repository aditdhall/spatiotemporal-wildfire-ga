{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# 0. CONFIG\n",
    "# ============================================================\n",
    "PROJECT_ID = \"nth-gasket-479617-q5\"   # <-- PUT YOUR EE PROJECT ID HERE\n",
    "\n",
    "TRAIN_CSV = \"original_data/wildfire_train.csv\"\n",
    "VAL_CSV   = \"original_data/wildfire_val.csv\"\n",
    "\n",
    "TRAIN_OUT_XLSX = \"original_data/wildfire_train_with_ndvi.xlsx\"\n",
    "VAL_OUT_XLSX   = \"original_data/wildfire_val_with_ndvi.xlsx\"\n",
    "\n",
    "TEST_CSV = \"original_data/wildfire_test.csv\"\n",
    "TEST_OUT_XLSX = \"original_data/wildfire_test_with_ndvi.xlsx\"\n",
    "TEST_PROGRESS_CSV = \"original_data/wildfire_test_with_ndvi_progress.csv\"\n",
    "\n",
    "\n",
    "# NEW: progress CSVs for faster autosave/resume\n",
    "TRAIN_PROGRESS_CSV = \"original_data/wildfire_train_with_ndvi_progress.csv\"\n",
    "VAL_PROGRESS_CSV   = \"original_data/wildfire_val_with_ndvi_progress.csv\"\n",
    "\n",
    "\n",
    "LAT_COL  = \"latitude\"\n",
    "LON_COL  = \"longitude\"\n",
    "DATE_COL = \"datetime\"\n",
    "NDVI_COL = \"ndvi_modis\"\n",
    "\n",
    "# autosave (rows with NEW NDVI)\n",
    "SAVE_EVERY = 200\n",
    "\n",
    "# Max number of points per Earth Engine batch\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# MODIS collection ID\n",
    "MODIS_IC = \"MODIS/061/MOD13Q1\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. Earth Engine init\n",
    "# ============================================================\n",
    "ee.Initialize(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Helper: load (with resume) train/val DataFrame\n",
    "# ============================================================\n",
    "def load_split_with_resume(csv_path, out_xlsx_path, progress_csv_path):\n",
    "    \"\"\"\n",
    "    Resume priority:\n",
    "      1) if progress CSV exists -> load that\n",
    "      2) elif XLSX exists -> load that\n",
    "      3) else -> load original CSV\n",
    "    \"\"\"\n",
    "    if os.path.exists(progress_csv_path):\n",
    "        print(f\"Found progress CSV: {progress_csv_path}, resuming from it.\")\n",
    "        df = pd.read_csv(progress_csv_path)\n",
    "    elif os.path.exists(out_xlsx_path):\n",
    "        print(f\"Found Excel file: {out_xlsx_path}, resuming from it.\")\n",
    "        df = pd.read_excel(out_xlsx_path)\n",
    "    else:\n",
    "        print(f\"Loading original CSV: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalize datetime\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Ensure NDVI column exists\n",
    "    if NDVI_COL not in df.columns:\n",
    "        df[NDVI_COL] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Helper: get MODIS NDVI image for a given date window\n",
    "# ============================================================\n",
    "def get_modis_image_for_date(date_str):\n",
    "    \"\"\"\n",
    "    Given a 'YYYY-MM-DD' date string, return the MOD13Q1 image\n",
    "    within ±8 days of that date. Returns None if not found.\n",
    "    \"\"\"\n",
    "    date = ee.Date(date_str)\n",
    "    start = date.advance(-8, \"day\")\n",
    "    end   = date.advance( 8, \"day\")\n",
    "\n",
    "    collection = (\n",
    "        ee.ImageCollection(MODIS_IC)\n",
    "        .filterDate(start, end)\n",
    "        .sort(\"system:time_start\")\n",
    "    )\n",
    "\n",
    "    image = collection.first()\n",
    "    return image\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Helper: fetch NDVI for a batch of rows (single date)\n",
    "# ============================================================\n",
    "def fetch_ndvi_batch_for_indices(df, indices, max_retries=3):\n",
    "    \"\"\"\n",
    "    For a subset of rows (given by indices) that share a single date,\n",
    "    build an EE FeatureCollection and sample NDVI in one call.\n",
    "\n",
    "    Returns: dict {row_index: ndvi_value or None}\n",
    "    \"\"\"\n",
    "    if len(indices) == 0:\n",
    "        return {}\n",
    "\n",
    "    sub = df.loc[indices]\n",
    "\n",
    "    # All rows in this batch must share the same date\n",
    "    unique_dates = sub[DATE_COL].unique()\n",
    "    if len(unique_dates) != 1:\n",
    "        raise ValueError(\"Batch must contain a single unique date.\")\n",
    "    date_str = unique_dates[0]\n",
    "\n",
    "    image = get_modis_image_for_date(date_str)\n",
    "    if image is None:\n",
    "        print(f\"No MODIS image found around date {date_str}.\")\n",
    "        return {idx: None for idx in indices}\n",
    "\n",
    "    # Build FeatureCollection with row_id property\n",
    "    features = []\n",
    "    for idx, row in sub.iterrows():\n",
    "        lon = float(row[LON_COL])\n",
    "        lat = float(row[LAT_COL])\n",
    "        geom = ee.Geometry.Point([lon, lat])\n",
    "        feat = ee.Feature(geom, {\"row_id\": int(idx)})\n",
    "        features.append(feat)\n",
    "\n",
    "    fc = ee.FeatureCollection(features)\n",
    "\n",
    "    # Sample NDVI at these points\n",
    "    sample = image.sampleRegions(\n",
    "        collection=fc,\n",
    "        scale=250,\n",
    "        properties=[\"row_id\"],\n",
    "        geometries=False\n",
    "    )\n",
    "\n",
    "    # Retrieve results with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = sample.getInfo()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"sampleRegions failed (attempt {attempt+1}) \"\n",
    "                  f\"for date {date_str}: {e}\")\n",
    "            time.sleep(2.0)\n",
    "    else:\n",
    "        # All retries failed\n",
    "        return {idx: None for idx in indices}\n",
    "\n",
    "    ndvi_map = {idx: None for idx in indices}\n",
    "    for feat in result.get(\"features\", []):\n",
    "        props = feat.get(\"properties\", {})\n",
    "        row_id = props.get(\"row_id\")\n",
    "        ndvi_val = props.get(\"NDVI\")\n",
    "        if row_id is not None:\n",
    "            ndvi_map[int(row_id)] = ndvi_val\n",
    "\n",
    "    return ndvi_map\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Main: process one split (train or val)\n",
    "# ============================================================\n",
    "def process_split(name, df, out_xlsx_path, progress_csv_path):\n",
    "\n",
    "    \"\"\"\n",
    "    name: 'train' or 'val' (for printing)\n",
    "    df: DataFrame with columns [LAT_COL, LON_COL, DATE_COL, NDVI_COL]\n",
    "    out_xlsx_path: where to save progress\n",
    "    \"\"\"\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\"Processing split: {name}\")\n",
    "    print(\"============================\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    # Consider only rows where NDVI is still missing\n",
    "    pending_mask = df[NDVI_COL].isna()\n",
    "    pending_indices = df[pending_mask].index.to_list()\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows needing NDVI: {len(pending_indices)}\")\n",
    "\n",
    "    if not pending_indices:\n",
    "        print(\"Nothing to do, NDVI already filled for this split.\")\n",
    "        return\n",
    "\n",
    "    # Group remaining rows by date to batch EE calls\n",
    "    pending_df = df.loc[pending_indices]\n",
    "    groups_by_date = pending_df.groupby(DATE_COL).groups  # {date: [idx1, idx2, ...]}\n",
    "\n",
    "    updated_since_save = 0\n",
    "    processed_rows = 0\n",
    "    total_pending = len(pending_indices)\n",
    "\n",
    "    for date_str, idx_list in groups_by_date.items():\n",
    "        # Process this date in chunks of CHUNK_SIZE\n",
    "        idx_list = list(idx_list)\n",
    "        for i in range(0, len(idx_list), CHUNK_SIZE):\n",
    "            chunk_indices = idx_list[i : i + CHUNK_SIZE]\n",
    "\n",
    "            print(\n",
    "                f\"[{name}] Date {date_str}, \"\n",
    "                f\"rows {processed_rows+1}–{processed_rows+len(chunk_indices)} \"\n",
    "                f\"of {total_pending}\"\n",
    "            )\n",
    "\n",
    "            ndvi_map = fetch_ndvi_batch_for_indices(df, chunk_indices)\n",
    "\n",
    "            # Update DataFrame\n",
    "            for ridx, ndvi_val in ndvi_map.items():\n",
    "                df.at[ridx, NDVI_COL] = ndvi_val\n",
    "                updated_since_save += 1\n",
    "                processed_rows += 1\n",
    "\n",
    "            # Autosave\n",
    "            if updated_since_save >= SAVE_EVERY:\n",
    "                print(f\"Autosaving progress (CSV) ...\")\n",
    "                df.to_csv(progress_csv_path, index=False)\n",
    "                updated_since_save = 0\n",
    "\n",
    "\n",
    "    # Final save\n",
    "    print(f\"Final save for {name}\")\n",
    "    # Always save CSV (fast, reliable)\n",
    "    df.to_csv(progress_csv_path, index=False)\n",
    "\n",
    "    # Try Excel as final nice output\n",
    "    try:\n",
    "        print(f\"Writing Excel to {out_xlsx_path} ...\")\n",
    "        df.to_excel(out_xlsx_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Excel save failed: {e}\")\n",
    "        print(\"But CSV progress is fully saved; you can convert to Excel later if needed.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Run for TRAIN + VAL\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train\n",
    "    train_df = load_split_with_resume(TRAIN_CSV, TRAIN_OUT_XLSX, TRAIN_PROGRESS_CSV)\n",
    "    process_split(\"train\", train_df, TRAIN_OUT_XLSX, TRAIN_PROGRESS_CSV)\n",
    "\n",
    "    # Val\n",
    "    val_df = load_split_with_resume(VAL_CSV, VAL_OUT_XLSX, VAL_PROGRESS_CSV)\n",
    "    process_split(\"val\", val_df, VAL_OUT_XLSX, VAL_PROGRESS_CSV)\n",
    "    \n",
    "    # Test\n",
    "    test_df = load_split_with_resume(TEST_CSV, TEST_OUT_XLSX, TEST_PROGRESS_CSV)\n",
    "    process_split(\"test\", test_df, TEST_OUT_XLSX, TEST_PROGRESS_CSV)\n",
    "\n",
    "    print(\"\\nAll done – NDVI added to train + val + test.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
